{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f2c16396",
      "metadata": {
        "id": "f2c16396"
      },
      "source": [
        "### Video Information Extracting with Qwen2.5-Omni\n",
        "\n",
        "This notebook demonstrates how to use Qwen2.5-Omni to obtain information from the video stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f59b3d9a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'defaults': ['_self_'],\n",
              " 'dataset': {'use_video_files': False,\n",
              "  'num_videos': 100,\n",
              "  'num_classes': 5,\n",
              "  'frames_per_video': 64,\n",
              "  'frame_height': 56,\n",
              "  'frame_width': 56,\n",
              "  'fps': 24,\n",
              "  'max_skip': 1,\n",
              "  'seed': 42,\n",
              "  'temp_folder': './temp_videos/',\n",
              "  'delete_videos': False},\n",
              " 'model': {'base_model': 'Qwen/Qwen2.5-Omni-7B',\n",
              "  'use_4bit': False,\n",
              "  'lora_r': 64,\n",
              "  'lora_alpha': 128,\n",
              "  'lora_target_modules': ['q_proj',\n",
              "   'v_proj',\n",
              "   'k_proj',\n",
              "   'o_proj',\n",
              "   'gate_proj',\n",
              "   'up_proj',\n",
              "   'down_proj'],\n",
              "  'lora_weights': 'qwen_video_lora',\n",
              "  'print_parameters': True},\n",
              " 'train': {'batch_size': 4,\n",
              "  'num_epochs': 8,\n",
              "  'learning_rate': 2e-05,\n",
              "  'precision': 'bf16',\n",
              "  'system_prompt': 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.',\n",
              "  'val_split': 0.2,\n",
              "  'eval_after_steps': 10000},\n",
              " 'wandb': {'project': 'qwen-video-classification',\n",
              "  'run_name': 'qwen_lora_run',\n",
              "  'mode': 'disabled'},\n",
              " 'demo': {'video_path': '/home/mykola/blacklab/qwen2.5-Omni-FT/tmp_videos/sample_75_6c12651f6c8f.mp4',\n",
              "  'base_model': 'Qwen/Qwen2.5-Omni-7B',\n",
              "  'lora_weights': 'qwen_video_lora',\n",
              "  'max_new_tokens': 100,\n",
              "  'use_synthetic_dataset': True,\n",
              "  'num_samples': 5}}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 1: Install if needed (optional)\n",
        "# !pip install torch transformers peft imageio opencv-python pillow qwen-omni-utils\n",
        "\n",
        "# Cell 2: Imports\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import Video, display\n",
        "import imageio\n",
        "import cv2\n",
        "\n",
        "from transformers import Qwen2_5OmniModel, Qwen2_5OmniProcessor, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from qwen_omni_utils import process_mm_info\n",
        "from generator import SyntheticVideoDataset  # Adjust if needed\n",
        "\n",
        "# Cell 1: Load OmegaConf config just like Hydra\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "cfg = OmegaConf.load(\"conf/config.yaml\")\n",
        "OmegaConf.to_container(cfg, resolve=True)  # To view if needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4004855e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
            "Qwen2_5OmniToken2WavModel must inference with fp32, but flash_attention_2 only supports fp16 and bf16, attention implementation of Qwen2_5OmniToken2WavModel will fallback to sdpa.\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.20it/s]\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load model using config\n",
        "from transformers import Qwen2_5OmniModel, Qwen2_5OmniProcessor, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = cfg.model.base_model\n",
        "lora_weights = cfg.model.lora_weights\n",
        "\n",
        "model = Qwen2_5OmniModel.from_pretrained(model_name, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n",
        "model = model.thinker\n",
        "\n",
        "model = PeftModel.from_pretrained(model, lora_weights)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "processor = Qwen2_5OmniProcessor.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386e4cd8",
      "metadata": {
        "id": "386e4cd8"
      },
      "source": [
        "Load model and processors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e829b782-0be7-4bc6-a576-6b815323376e",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "9d23feb62f9a42428b3110ee4bc4d9d7",
            "8c875aaa96f4426190cbf8875bdaae4a",
            "fe4d97abddf945bb90153a38ea75f307",
            "630e754c19764b2b9ea7c92b4a73399a",
            "8f65cb1de308412b80587f559f807a72",
            "d75d90b36a7f4a4c92cba80209f3262f",
            "e27da40a1a754ee9b8fbc4fd94091173",
            "e0b4d068845545ecba379d9c7bdf8b5e",
            "ec60b9c993564848be36afd5fd64e3d9",
            "5e61dc8f855540279325c55ad01c3bf1",
            "c2bdaa908d2e41ac97ed46888a5468fd"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-01-29T12:40:18.337731Z",
          "iopub.status.busy": "2025-01-29T12:40:18.337470Z",
          "iopub.status.idle": "2025-01-29T12:40:47.760976Z",
          "shell.execute_reply": "2025-01-29T12:40:47.760220Z",
          "shell.execute_reply.started": "2025-01-29T12:40:18.337713Z"
        },
        "id": "e829b782-0be7-4bc6-a576-6b815323376e",
        "outputId": "93f38a2a-a235-4954-9ed9-1bb76ada26ff",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (56, 56) to (64, 64) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Load or generate video\n",
        "from generator import SyntheticVideoDataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "def save_video(frames, path=None, fps=None):\n",
        "    if path is None:\n",
        "        path = \"synthetic.mp4\"\n",
        "    if fps is None:\n",
        "        fps = 24  \n",
        "    \n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    writer = imageio.get_writer(path, fps=fps, codec=\"libx264\", format=\"FFMPEG\")\n",
        "    for f in frames:\n",
        "        writer.append_data(np.array(f.convert(\"RGB\")))\n",
        "    writer.close()\n",
        "\n",
        "dataset = SyntheticVideoDataset(\n",
        "    num_videos=cfg.dataset.num_videos,\n",
        "    num_classes=cfg.dataset.num_classes,\n",
        "    frames_per_video=cfg.dataset.frames_per_video,\n",
        "    frame_height=cfg.dataset.frame_height,\n",
        "    frame_width=cfg.dataset.frame_width,\n",
        "    max_skip=cfg.dataset.max_skip,\n",
        "    seed=None\n",
        ")\n",
        "import random\n",
        "index = random.randint(0, len(dataset) - 1)\n",
        "frames, label_id = dataset[index]\n",
        "label_text = dataset.classes[label_id]\n",
        "video_path = os.path.join(cfg.dataset.temp_folder, \"demo_video.mp4\")\n",
        "save_video(frames,  video_path, fps=cfg.dataset.fps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "333458da",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video src=\"./temp_videos/demo_video.mp4\" controls  width=\"320\"  height=\"320\">\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ],
            "text/plain": [
              "<IPython.core.display.Video object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cell 4: Display the video\n",
        "from IPython.display import Video, display\n",
        "\n",
        "display(Video(video_path, width=320, height=320))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "036fbfb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Inference function\n",
        "from qwen_omni_utils import process_mm_info\n",
        "\n",
        "def run_inference(model, processor, sys_prompt, prompt_text, video_frames, device, max_new_tokens=100):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt_text},\n",
        "            {\"type\": \"video\", \"video\": video_frames},\n",
        "        ]}\n",
        "    ]\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    audios, images, videos = process_mm_info(messages, use_audio_in_video=False)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=text,\n",
        "        audios=audios,\n",
        "        images=images,\n",
        "        videos=videos,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        use_audio_in_video=False\n",
        "    )\n",
        "    inputs = inputs.to(device).to(model.dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "\n",
        "    return processor.batch_decode(output, skip_special_tokens=True)[0].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6d72d5e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§  Ground Truth: horizontal move\n",
            "ðŸ¤– Model Prediction: system\n",
            "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
            "user\n",
            "Please classify this video. Choose one of: horizontal_move, vertical_move, blinking_dot, random_teleport, bouncing_diag. Respond with only the label.\n",
            "assistant\n",
            "horizontal_move\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Final run\n",
        "sys_prompt = cfg.train.system_prompt\n",
        "prompt_text = \"Please classify this video. Choose one of: horizontal_move, vertical_move, blinking_dot, random_teleport, bouncing_diag. Respond with only the label.\"\n",
        "\n",
        "prediction = run_inference(model, processor, sys_prompt, prompt_text, frames, device, cfg.demo.max_new_tokens)\n",
        "\n",
        "print(f\"ðŸ§  Ground Truth: {label_text}\")\n",
        "print(f\"ðŸ¤– Model Prediction: {prediction}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e61dc8f855540279325c55ad01c3bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630e754c19764b2b9ea7c92b4a73399a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e61dc8f855540279325c55ad01c3bf1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c2bdaa908d2e41ac97ed46888a5468fd",
            "value": "â€‡5/5â€‡[00:10&lt;00:00,â€‡â€‡2.03s/it]"
          }
        },
        "8c875aaa96f4426190cbf8875bdaae4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d75d90b36a7f4a4c92cba80209f3262f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e27da40a1a754ee9b8fbc4fd94091173",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "8f65cb1de308412b80587f559f807a72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d23feb62f9a42428b3110ee4bc4d9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c875aaa96f4426190cbf8875bdaae4a",
              "IPY_MODEL_fe4d97abddf945bb90153a38ea75f307",
              "IPY_MODEL_630e754c19764b2b9ea7c92b4a73399a"
            ],
            "layout": "IPY_MODEL_8f65cb1de308412b80587f559f807a72"
          }
        },
        "c2bdaa908d2e41ac97ed46888a5468fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d75d90b36a7f4a4c92cba80209f3262f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0b4d068845545ecba379d9c7bdf8b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e27da40a1a754ee9b8fbc4fd94091173": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec60b9c993564848be36afd5fd64e3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe4d97abddf945bb90153a38ea75f307": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0b4d068845545ecba379d9c7bdf8b5e",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec60b9c993564848be36afd5fd64e3d9",
            "value": 5
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
